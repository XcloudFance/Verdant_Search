#!/usr/bin/env python3
"""
æ¸…ç†æ‰€æœ‰æœç´¢å¼•æ“æ•°æ®ï¼ˆè‡ªåŠ¨æ‰§è¡Œç‰ˆæœ¬ï¼‰
"""
import asyncio
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy import text
from database import engine, AsyncSessionLocal
from models import Base
import redis

# å¯¼å…¥çˆ¬è™«é…ç½®
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'crawler'))
from crawler_config import (
    REDIS_HOST, REDIS_PORT, REDIS_CRAWLER_DB, REDIS_PASSWORD,
    BLOOMFILTER_KEY, TASK_QUEUE_KEY
)

async def drop_all_tables():
    """åˆ é™¤æ‰€æœ‰æ•°æ®åº“è¡¨"""
    print("\nğŸ—‘ï¸  åˆ é™¤æ‰€æœ‰æ•°æ®åº“è¡¨...")
    
    async with engine.begin() as conn:
        # ä½¿ç”¨ CASCADE åˆ é™¤æ‰€æœ‰è¡¨åŠå…¶ä¾èµ–
        await conn.execute(text("DROP SCHEMA public CASCADE"))
        await conn.execute(text("CREATE SCHEMA public"))
        print("âœ… æ‰€æœ‰è¡¨å·²åˆ é™¤")

async def create_all_tables():
    """é‡æ–°åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨"""
    print("\nğŸ”¨ é‡æ–°åˆ›å»ºæ•°æ®åº“è¡¨...")
    
    async with engine.begin() as conn:
        # å…ˆåˆ›å»º pgvector æ‰©å±•
        await conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
        # åˆ›å»ºæ‰€æœ‰è¡¨
        await conn.run_sync(Base.metadata.create_all)
        print("âœ… æ‰€æœ‰è¡¨å·²åˆ›å»º")

def clear_redis_data():
    """æ¸…ç©º Redis ä¸­çš„çˆ¬è™«æ•°æ®"""
    print("\nğŸ§¹ æ¸…ç©º Redis çˆ¬è™«æ•°æ®...")
    
    try:
        redis_client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_CRAWLER_DB,
            password=REDIS_PASSWORD if REDIS_PASSWORD else None,
            decode_responses=False
        )
        
        redis_client.ping()
        print(f"âœ… å·²è¿æ¥åˆ° Redis: {REDIS_HOST}:{REDIS_PORT}/{REDIS_CRAWLER_DB}")
        
        # è·å–æ‰€æœ‰çˆ¬è™«ç›¸å…³çš„é”®
        crawler_keys = redis_client.keys("crawler:*")
        
        if crawler_keys:
            print(f"æ‰¾åˆ° {len(crawler_keys)} ä¸ªçˆ¬è™«ç›¸å…³çš„é”®")
            deleted = redis_client.delete(*crawler_keys)
            print(f"âœ… å·²åˆ é™¤ {deleted} ä¸ª Redis é”®")
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°çˆ¬è™«ç›¸å…³çš„ Redis é”®")
        
        # é¢å¤–ç¡®ä¿åˆ é™¤ Bloomfilter å’Œä»»åŠ¡é˜Ÿåˆ—
        redis_client.delete(BLOOMFILTER_KEY)
        redis_client.delete(TASK_QUEUE_KEY)
        
        print("âœ… Redis æ•°æ®å·²æ¸…ç©º")
        
    except Exception as e:
        print(f"âŒ æ¸…ç©º Redis æ•°æ®å¤±è´¥: {e}")
        raise

async def verify_cleanup():
    """éªŒè¯æ¸…ç†ç»“æœ"""
    print("\nğŸ” éªŒè¯æ¸…ç†ç»“æœ...")
    
    async with AsyncSessionLocal() as session:
        from sqlalchemy import text
        
        tables = ["documents", "document_embeddings", "image_embeddings", "terms", "postings", "doc_stats"]
        
        for table in tables:
            result = await session.execute(text(f"SELECT COUNT(*) FROM {table}"))
            count = result.scalar()
            print(f"  - {table}: {count} è¡Œ")
        
        print("âœ… éªŒè¯å®Œæˆ")

async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("å¼€å§‹æ¸…ç†æ‰€æœ‰æœç´¢å¼•æ“æ•°æ®...")
    print("="*60)
    
    try:
        # 1. åˆ é™¤æ‰€æœ‰è¡¨
        await drop_all_tables()
        
        # 2. æ¸…ç©º Redis
        clear_redis_data()
        
        # 3. é‡æ–°åˆ›å»ºè¡¨
        await create_all_tables()
        
        # 4. éªŒè¯
        await verify_cleanup()
        
        print("\n" + "="*60)
        print("âœ… æ¸…ç†å®Œæˆï¼")
        print("="*60)
        print("\nç°åœ¨å¯ä»¥é‡æ–°å¼€å§‹çˆ¬å–æ•°æ®äº†ã€‚")
        print("ä½¿ç”¨å‘½ä»¤: bash start_crawler.sh <URL>\n")
        
    except Exception as e:
        print(f"\nâŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
