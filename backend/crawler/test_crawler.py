#!/usr/bin/env python3
"""
çˆ¬è™«ç»„ä»¶æµ‹è¯•è„šæœ¬
"""
import sys
import os
# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from url_manager import URLManager
from content_extractor import ContentExtractor
import requests


def test_redis_connection():
    """æµ‹è¯•Redisè¿æ¥"""
    print("æµ‹è¯• Redis è¿æ¥...")
    try:
        manager = URLManager()
        print(f"âœ“ Redis è¿æ¥æˆåŠŸ")
        print(f"  - é˜Ÿåˆ—å¤§å°: {manager.get_queue_size()}")
        print(f"  - å·²è®¿é—®URLæ•°: {manager.get_visited_count()}")
        return True
    except Exception as e:
        print(f"âœ— Redis è¿æ¥å¤±è´¥: {e}")
        return False


def test_bloomfilter():
    """æµ‹è¯•Bloomfilter"""
    print("\næµ‹è¯• Bloomfilter...")
    try:
        manager = URLManager()
        
        # æµ‹è¯•URL
        test_url = "https://test.example.com/page1"
        
        # æ£€æŸ¥åˆå§‹çŠ¶æ€
        is_visited = manager.is_visited(test_url)
        print(f"  - URLåˆå§‹çŠ¶æ€: {'å·²è®¿é—®' if is_visited else 'æœªè®¿é—®'}")
        
        # æ ‡è®°ä¸ºå·²è®¿é—®
        manager.mark_visited(test_url)
        
        # å†æ¬¡æ£€æŸ¥
        is_visited = manager.is_visited(test_url)
        print(f"  - æ ‡è®°åçŠ¶æ€: {'å·²è®¿é—®' if is_visited else 'æœªè®¿é—®'}")
        
        if is_visited:
            print(f"âœ“ Bloomfilter å·¥ä½œæ­£å¸¸")
            return True
        else:
            print(f"âœ— Bloomfilter æœªæ­£å¸¸å·¥ä½œ")
            return False
    except Exception as e:
        print(f"âœ— Bloomfilter æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_url_validation():
    """æµ‹è¯•URLéªŒè¯"""
    print("\næµ‹è¯• URL éªŒè¯...")
    manager = URLManager()
    
    test_cases = [
        ("https://www.example.com/", True),
        ("http://example.com/page", True),
        ("ftp://example.com/", False),
        ("https://example.com/image.jpg", False),
        ("https://example.com/doc.pdf", False),
        ("not-a-url", False),
    ]
    
    all_passed = True
    for url, expected in test_cases:
        result = manager.is_valid_url(url)
        status = "âœ“" if result == expected else "âœ—"
        print(f"  {status} {url}: {result} (æœŸæœ›: {expected})")
        if result != expected:
            all_passed = False
    
    if all_passed:
        print(f"âœ“ URLéªŒè¯æµ‹è¯•é€šè¿‡")
    else:
        print(f"âœ— URLéªŒè¯æµ‹è¯•å¤±è´¥")
    
    return all_passed


def test_content_extraction():
    """æµ‹è¯•å†…å®¹æå–"""
    print("\næµ‹è¯•å†…å®¹æå–...")
    
    html = """
    <html>
    <head>
        <title>Test Page</title>
        <meta property="og:title" content="OG Test Title">
    </head>
    <body>
        <h1>Main Title</h1>
        <article>
            <p>This is the main content of the page.</p>
            <p>It has multiple paragraphs.</p>
        </article>
        <script>console.log('test');</script>
    </body>
    </html>
    """
    
    try:
        extractor = ContentExtractor()
        result = extractor.extract(html, "https://test.example.com")
        
        if result:
            print(f"âœ“ å†…å®¹æå–æˆåŠŸ")
            print(f"  - æ ‡é¢˜: {result['title']}")
            print(f"  - å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
            print(f"  - å†…å®¹é¢„è§ˆ: {result['content'][:100]}...")
            return True
        else:
            print(f"âœ— å†…å®¹æå–å¤±è´¥")
            return False
    except Exception as e:
        print(f"âœ— å†…å®¹æå–æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_link_extraction():
    """æµ‹è¯•é“¾æ¥æå–"""
    print("\næµ‹è¯•é“¾æ¥æå–...")
    
    html = """
    <html>
    <body>
        <a href="https://example.com/page1">Link 1</a>
        <a href="/relative/page">Relative Link</a>
        <a href="image.jpg">Image</a>
        <a href="https://other.com/page">External Link</a>
    </body>
    </html>
    """
    
    try:
        manager = URLManager()
        links = manager.extract_links("https://example.com/base", html)
        
        print(f"âœ“ æå–åˆ° {len(links)} ä¸ªé“¾æ¥:")
        for link in links:
            print(f"  - {link}")
        
        return True
    except Exception as e:
        print(f"âœ— é“¾æ¥æå–æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_task_queue():
    """æµ‹è¯•ä»»åŠ¡é˜Ÿåˆ—"""
    print("\næµ‹è¯•ä»»åŠ¡é˜Ÿåˆ—...")
    
    try:
        manager = URLManager()
        
        # æ·»åŠ æµ‹è¯•URL
        test_urls = [
            "https://test1.example.com/",
            "https://test2.example.com/",
            "https://test3.example.com/",
        ]
        
        print(f"æ·»åŠ  {len(test_urls)} ä¸ªæµ‹è¯•URL...")
        added = manager.add_urls(test_urls, depth=0)
        print(f"æˆåŠŸæ·»åŠ  {added} ä¸ªURL")
        
        # è·å–URL
        print(f"ä»é˜Ÿåˆ—è·å–URL...")
        for i in range(min(added, 3)):
            task = manager.get_next_url()
            if task:
                print(f"  - {task['url']} (depth={task['depth']})")
        
        print(f"âœ“ ä»»åŠ¡é˜Ÿåˆ—æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âœ— ä»»åŠ¡é˜Ÿåˆ—æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_http_request():
    """æµ‹è¯•HTTPè¯·æ±‚"""
    print("\næµ‹è¯• HTTP è¯·æ±‚...")
    
    try:
        from crawler import CrawlerWorker
        from multiprocessing import Event
        
        worker = CrawlerWorker(0, Event())
        html = worker.fetch_page("https://www.example.com/")
        
        if html:
            print(f"âœ“ HTTPè¯·æ±‚æˆåŠŸ")
            print(f"  - å“åº”é•¿åº¦: {len(html)} å­—ç¬¦")
            return True
        else:
            print(f"âœ— HTTPè¯·æ±‚å¤±è´¥")
            return False
    except Exception as e:
        print(f"âœ— HTTPè¯·æ±‚æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    print("=" * 60)
    print("çˆ¬è™«ç»„ä»¶æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("Redisè¿æ¥", test_redis_connection),
        ("Bloomfilter", test_bloomfilter),
        ("URLéªŒè¯", test_url_validation),
        ("å†…å®¹æå–", test_content_extraction),
        ("é“¾æ¥æå–", test_link_extraction),
        ("ä»»åŠ¡é˜Ÿåˆ—", test_task_queue),
        ("HTTPè¯·æ±‚", test_http_request),
    ]
    
    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\næµ‹è¯• '{name}' å‡ºç°å¼‚å¸¸: {e}")
            results.append((name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{status}: {name}")
    
    print("-" * 60)
    print(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        return 1


if __name__ == '__main__':
    exit(main())
