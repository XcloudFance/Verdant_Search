"""
URLç®¡ç†å™¨ - ä½¿ç”¨Redis Bloomfilterå»é‡å’Œä»»åŠ¡é˜Ÿåˆ—
"""
import redis
from typing import List, Optional
from urllib.parse import urlparse, urljoin
import logging
from pybloom_live import BloomFilter
import pickle

from crawler_config import (
    REDIS_HOST, REDIS_PORT, REDIS_CRAWLER_DB, REDIS_PASSWORD,
    BLOOMFILTER_KEY, TASK_QUEUE_KEY,
    BLOOMFILTER_ERROR_RATE, BLOOMFILTER_CAPACITY,
    EXCLUDED_EXTENSIONS, ALLOWED_DOMAINS,
    ENABLE_RATE_LIMIT, REQUESTS_PER_BATCH, BATCH_REST_DURATION
)

logger = logging.getLogger(__name__)


class URLManager:
    """URLç®¡ç†å™¨ - ä½¿ç”¨Redis Bloomfilterå»é‡"""
    
    def __init__(self):
        # è¿æ¥Redis
        self.redis_client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_CRAWLER_DB,
            password=REDIS_PASSWORD if REDIS_PASSWORD else None,
            decode_responses=False  # Bloomfilteréœ€è¦bytes
        )
        
        # åˆå§‹åŒ–æˆ–åŠ è½½Bloomfilter
        self.bloom_filter = self._load_or_create_bloom_filter()
        
        logger.info(f"URLManager initialized with Redis at {REDIS_HOST}:{REDIS_PORT}/{REDIS_CRAWLER_DB}")
    
    def _load_or_create_bloom_filter(self) -> BloomFilter:
        """ä»RedisåŠ è½½æˆ–åˆ›å»ºæ–°çš„Bloomfilter"""
        try:
            bloom_data = self.redis_client.get(BLOOMFILTER_KEY)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                logger.info(f"Loaded existing Bloomfilter with {bloom_filter.count} items")
                return bloom_filter
        except Exception as e:
            logger.warning(f"Failed to load Bloomfilter: {e}")
        
        # åˆ›å»ºæ–°çš„Bloomfilter
        bloom_filter = BloomFilter(capacity=BLOOMFILTER_CAPACITY, error_rate=BLOOMFILTER_ERROR_RATE)
        logger.info(f"Created new Bloomfilter (capacity={BLOOMFILTER_CAPACITY}, error_rate={BLOOMFILTER_ERROR_RATE})")
        return bloom_filter
    
    def _save_bloom_filter(self):
        """ä¿å­˜Bloomfilteråˆ°Redis"""
        try:
            bloom_data = pickle.dumps(self.bloom_filter)
            self.redis_client.set(BLOOMFILTER_KEY, bloom_data)
        except Exception as e:
            logger.error(f"Failed to save Bloomfilter: {e}")
    
    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        # ç§»é™¤URL fragment
        parsed = urlparse(url)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        return normalized.rstrip('/')
    
    def is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            
            # æ£€æŸ¥åè®®
            if parsed.scheme not in ['http', 'https']:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸŸå
            if not parsed.netloc:
                return False
            
            # æ£€æŸ¥æ‰©å±•å
            path = parsed.path.lower()
            if any(path.endswith(ext) for ext in EXCLUDED_EXTENSIONS):
                return False
            
            # æ£€æŸ¥åŸŸåç™½åå•
            if ALLOWED_DOMAINS:
                domain = parsed.netloc.lower()
                if not any(allowed in domain for allowed in ALLOWED_DOMAINS):
                    return False
            
            return True
        except Exception:
            return False
    
    def is_visited(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²è®¿é—®ï¼ˆå®æ—¶ä»RedisåŠ è½½ï¼‰"""
        normalized_url = self.normalize_url(url)
        
        # å®æ—¶ä» Redis åŠ è½½æœ€æ–°çš„ Bloomfilter
        try:
            bloom_data = self.redis_client.get(BLOOMFILTER_KEY)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                return normalized_url in bloom_filter
        except Exception as e:
            logger.warning(f"Failed to load Bloomfilter for check: {e}")
        
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜
        return normalized_url in self.bloom_filter
    
    def mark_visited(self, url: str):
        """æ ‡è®°URLä¸ºå·²è®¿é—®ï¼ˆå®æ—¶ä¿å­˜åˆ°Redisï¼‰"""
        normalized_url = self.normalize_url(url)
        
        # ä½¿ç”¨ Redis é”ç¡®ä¿åŸå­æ€§
        lock_key = f"{BLOOMFILTER_KEY}:lock"
        lock = self.redis_client.lock(lock_key, timeout=5)
        
        try:
            # è·å–é”
            if lock.acquire(blocking=True, blocking_timeout=10):
                try:
                    # ä» Redis åŠ è½½æœ€æ–°çš„ Bloomfilter
                    bloom_data = self.redis_client.get(BLOOMFILTER_KEY)
                    if bloom_data:
                        bloom_filter = pickle.loads(bloom_data)
                    else:
                        bloom_filter = BloomFilter(capacity=BLOOMFILTER_CAPACITY, error_rate=BLOOMFILTER_ERROR_RATE)
                    
                    # æ·»åŠ  URL
                    bloom_filter.add(normalized_url)
                    
                    # ä¿å­˜å› Redis
                    bloom_data = pickle.dumps(bloom_filter)
                    self.redis_client.set(BLOOMFILTER_KEY, bloom_data)
                    
                    # æ›´æ–°æœ¬åœ°ç¼“å­˜
                    self.bloom_filter = bloom_filter
                    
                finally:
                    # é‡Šæ”¾é”
                    lock.release()
            else:
                logger.warning(f"Failed to acquire lock for marking URL as visited: {url}")
        except Exception as e:
            logger.error(f"Failed to mark URL as visited: {e}")
    
    def check_and_rest_if_needed(self):
        """
        æ£€æŸ¥å…¨å±€è¯·æ±‚è®¡æ•°ï¼Œå¿…è¦æ—¶ä¼‘æ¯
        
        æ¯å¤„ç† REQUESTS_PER_BATCH ä¸ªè¯·æ±‚åï¼Œæ‰€æœ‰workerç»Ÿä¸€ä¼‘æ¯ BATCH_REST_DURATION ç§’
        ä½¿ç”¨ Redis å®ç°åˆ†å¸ƒå¼è®¡æ•°å’ŒåŒæ­¥
        """
        # å¦‚æœé™é€ŸåŠŸèƒ½è¢«ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        if not ENABLE_RATE_LIMIT:
            return
        
        import time
        
        # Redis keys
        request_counter_key = "crawler:request_counter"
        rest_until_key = "crawler:rest_until"
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨ä¼‘æ¯
        rest_until = self.redis_client.get(rest_until_key)
        if rest_until:
            rest_until_time = float(rest_until.decode('utf-8'))
            now = time.time()
            if now < rest_until_time:
                # æ­£åœ¨ä¼‘æ¯ä¸­
                sleep_time = rest_until_time - now
                logger.info(f"ğŸ›‘ å…¨å±€é™é€Ÿ: ä¼‘æ¯ä¸­ï¼Œè¿˜éœ€ç­‰å¾… {int(sleep_time)} ç§’...")
                time.sleep(sleep_time)
                return
        
        # åŸå­æ€§é€’å¢è¯·æ±‚è®¡æ•°
        current_count = self.redis_client.incr(request_counter_key)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡é™åˆ¶
        if current_count >= REQUESTS_PER_BATCH:
            # è®¾ç½®ä¼‘æ¯æ—¶é—´
            rest_until_time = time.time() + BATCH_REST_DURATION
            self.redis_client.set(rest_until_key, str(rest_until_time), ex=BATCH_REST_DURATION + 10)
            
            # é‡ç½®è®¡æ•°å™¨
            self.redis_client.set(request_counter_key, "0")
            
            logger.warning(
                f"â¸ï¸  å…¨å±€é™é€Ÿ: å·²å¤„ç† {current_count} ä¸ªè¯·æ±‚ï¼Œ"
                f"ä¼‘æ¯ {BATCH_REST_DURATION} ç§’..."
            )
            time.sleep(BATCH_REST_DURATION)
            logger.info("â–¶ï¸  å…¨å±€é™é€Ÿ: ä¼‘æ¯ç»“æŸï¼Œç»§ç»­çˆ¬å–")
    
    def add_urls(self, urls: List[str], depth: int = 0):
        """æ‰¹é‡æ·»åŠ URLåˆ°ä»»åŠ¡é˜Ÿåˆ—"""
        added_count = 0
        for url in urls:
            if self.add_url(url, depth):
                added_count += 1
        
        logger.info(f"Added {added_count}/{len(urls)} URLs to task queue")
        return added_count
    
    def add_url(self, url: str, depth: int = 0) -> bool:
        """æ·»åŠ å•ä¸ªURLåˆ°ä»»åŠ¡é˜Ÿåˆ—"""
        # éªŒè¯URL
        if not self.is_valid_url(url):
            return False
        
        normalized_url = self.normalize_url(url)
        
        # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
        if self.is_visited(normalized_url):
            return False
        
        # æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—ï¼ˆä½¿ç”¨ JSON æ ¼å¼ï¼‰
        try:
            import json
            task_data = {
                'url': normalized_url,
                'depth': depth
            }
            # ç»Ÿä¸€ä½¿ç”¨ JSON æ ¼å¼ï¼ˆæ›´é€šç”¨ã€æ˜“è°ƒè¯•ï¼‰
            self.redis_client.rpush(TASK_QUEUE_KEY, json.dumps(task_data))
            return True
        except Exception as e:
            logger.error(f"Failed to add URL to queue: {e}")
            return False
    
    def get_next_url(self) -> Optional[dict]:
        """ä»ä»»åŠ¡é˜Ÿåˆ—è·å–ä¸‹ä¸€ä¸ªURLï¼ˆå…¼å®¹ JSON å’Œ pickle æ ¼å¼ï¼‰"""
        try:
            import json
            task_data = self.redis_client.lpop(TASK_QUEUE_KEY)
            if not task_data:
                return None
            
            # ä¼˜å…ˆå°è¯• JSON æ ¼å¼ï¼ˆæ–°æ ¼å¼ï¼‰
            try:
                if isinstance(task_data, bytes):
                    task_data = task_data.decode('utf-8')
                return json.loads(task_data)
            except (json.JSONDecodeError, UnicodeDecodeError, AttributeError):
                # JSON å¤±è´¥ï¼Œå°è¯• pickle æ ¼å¼ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                try:
                    if isinstance(task_data, str):
                        task_data = task_data.encode('utf-8')
                    return pickle.loads(task_data)
                except Exception as pickle_error:
                    logger.error(f"Failed to deserialize task (tried JSON and pickle): {pickle_error}")
                    return None
        except Exception as e:
            logger.error(f"Failed to get URL from queue: {e}")
            return None
    
    def get_queue_size(self) -> int:
        """è·å–ä»»åŠ¡é˜Ÿåˆ—å¤§å°"""
        try:
            return self.redis_client.llen(TASK_QUEUE_KEY)
        except Exception:
            return 0
    
    def get_visited_count(self) -> int:
        """è·å–å·²è®¿é—®URLæ•°é‡ï¼ˆå®æ—¶ä»RedisåŠ è½½ï¼‰"""
        try:
            bloom_data = self.redis_client.get(BLOOMFILTER_KEY)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                return bloom_filter.count
        except Exception as e:
            logger.warning(f"Failed to get visited count from Redis: {e}")
        
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›æœ¬åœ°ç¼“å­˜çš„è®¡æ•°
        return self.bloom_filter.count
    
    def extract_links(self, base_url: str, html_content: str) -> List[str]:
        """ä»HTMLä¸­æå–æ‰€æœ‰é“¾æ¥"""
        from bs4 import BeautifulSoup
        
        links = []
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                # è½¬æ¢ç›¸å¯¹URLä¸ºç»å¯¹URL
                absolute_url = urljoin(base_url, href)
                
                if self.is_valid_url(absolute_url):
                    links.append(absolute_url)
        
        except Exception as e:
            logger.error(f"Failed to extract links from {base_url}: {e}")
        
        return links
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ˆå¼€å‘/è°ƒè¯•ç”¨ï¼‰"""
        try:
            self.redis_client.delete(BLOOMFILTER_KEY)
            self.redis_client.delete(TASK_QUEUE_KEY)
            self.bloom_filter = BloomFilter(capacity=BLOOMFILTER_CAPACITY, error_rate=BLOOMFILTER_ERROR_RATE)
            logger.info("Cleared all URL data")
        except Exception as e:
            logger.error(f"Failed to clear URL data: {e}")
    
    def save(self):
        """ä¿å­˜å½“å‰çŠ¶æ€"""
        self._save_bloom_filter()
        logger.info(f"Saved Bloomfilter with {self.bloom_filter.count} items")
