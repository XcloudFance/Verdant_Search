#!/usr/bin/env python3
"""
æ¸…ç† "The heart of the internet" æ•°æ®å¹¶é‡æ–°çˆ¬å–

åŠŸèƒ½:
1. ä»æ•°æ®åº“æ‰¾åˆ°æ‰€æœ‰ title="The heart of the internet" çš„æ–‡æ¡£
2. æå–è¿™äº›æ–‡æ¡£çš„ URL
3. å°† URL æ·»åŠ åˆ° Redis çˆ¬è™«é˜Ÿåˆ—çš„**å¤´éƒ¨**ï¼ˆä¼˜å…ˆçˆ¬å–ï¼‰
4. ä»æ•°æ®åº“**å½»åº•åˆ é™¤**è¿™äº›æ–‡æ¡£åŠç›¸å…³æ•°æ®
"""

import asyncio
import sys
import os
import redis
import json

# æ·»åŠ  python ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'python'))

from sqlalchemy import text
from database import AsyncSessionLocal

# Redis é…ç½®
REDIS_HOST = "localhost"
REDIS_PORT = 6379
REDIS_CRAWLER_DB = 1
TASK_QUEUE_KEY = "crawler:task_queue"


async def find_bad_documents():
    """æŸ¥æ‰¾æ‰€æœ‰ title åŒ…å« 'The heart of the internet' çš„æ–‡æ¡£"""
    print("ğŸ” æŸ¥æ‰¾éœ€è¦é‡æ–°çˆ¬å–çš„æ–‡æ¡£...")
    
    async with AsyncSessionLocal() as session:
        query = text("""
            SELECT id, title, url, created_at
            FROM documents
            WHERE title = 'The heart of the internet'
            OR title LIKE '%heart of the internet%'
            ORDER BY id
        """)
        
        result = await session.execute(query)
        docs = result.fetchall()
        
        return [
            {
                'id': row[0],
                'title': row[1],
                'url': row[2],
                'created_at': str(row[3])
            }
            for row in docs
        ]


async def delete_document_completely(doc_id: int, session):
    """å®Œå…¨åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®"""
    # 1. åˆ é™¤ postings (ä¼šè‡ªåŠ¨é€šè¿‡ CASCADE åˆ é™¤)
    # 2. åˆ é™¤ embeddings (ä¼šè‡ªåŠ¨é€šè¿‡ CASCADE åˆ é™¤)
    # 3. åˆ é™¤ document
    
    delete_query = text("""
        DELETE FROM documents WHERE id = :doc_id
    """)
    
    await session.execute(delete_query, {"doc_id": doc_id})


async def delete_bad_documents(doc_ids: list):
    """æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
    print(f"ğŸ—‘ï¸  å¼€å§‹åˆ é™¤ {len(doc_ids)} ä¸ªæ–‡æ¡£...")
    
    async with AsyncSessionLocal() as session:
        deleted = 0
        for doc_id in doc_ids:
            try:
                await delete_document_completely(doc_id, session)
                deleted += 1
                if deleted % 10 == 0:
                    print(f"   å·²åˆ é™¤ {deleted}/{len(doc_ids)}...")
            except Exception as e:
                print(f"   âŒ åˆ é™¤æ–‡æ¡£ {doc_id} å¤±è´¥: {e}")
        
        await session.commit()
        print(f"âœ… æˆåŠŸåˆ é™¤ {deleted} ä¸ªæ–‡æ¡£")


def add_urls_to_queue_head(urls: list):
    """å°† URL æ·»åŠ åˆ° Redis é˜Ÿåˆ—å¤´éƒ¨ï¼ˆä½¿ç”¨ LPUSHï¼‰"""
    print(f"ğŸ“¥ å°† {len(urls)} ä¸ª URL æ·»åŠ åˆ°çˆ¬è™«é˜Ÿåˆ—å¤´éƒ¨...")
    
    try:
        r = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_CRAWLER_DB,
            decode_responses=True
        )
        
        # ä½¿ç”¨ LPUSH æ·»åŠ åˆ°é˜Ÿåˆ—å¤´éƒ¨ï¼ˆä¼˜å…ˆçˆ¬å–ï¼‰
        added = 0
        for url in urls:
            if url:  # ç¡®ä¿ URL ä¸ä¸ºç©º
                task = json.dumps({
                    'url': url,
                    'depth': 0  # é‡ç½®æ·±åº¦
                })
                r.lpush(TASK_QUEUE_KEY, task)
                added += 1
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {added} ä¸ª URL åˆ°é˜Ÿåˆ—å¤´éƒ¨")
        
        # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
        queue_size = r.llen(TASK_QUEUE_KEY)
        print(f"ğŸ“Š å½“å‰é˜Ÿåˆ—å¤§å°: {queue_size}")
        
    except Exception as e:
        print(f"âŒ æ·»åŠ åˆ° Redis å¤±è´¥: {e}")
        raise


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("æ¸…ç†å¹¶é‡æ–°çˆ¬å–è„šæœ¬")
    print("=" * 70)
    print()
    
    # 1. æŸ¥æ‰¾éœ€è¦å¤„ç†çš„æ–‡æ¡£
    bad_docs = await find_bad_documents()
    
    if not bad_docs:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡æ–°çˆ¬å–çš„æ–‡æ¡£ï¼")
        return
    
    print(f"\næ‰¾åˆ° {len(bad_docs)} ä¸ªéœ€è¦é‡æ–°çˆ¬å–çš„æ–‡æ¡£:")
    print()
    
    # æ˜¾ç¤ºå‰10ä¸ª
    for i, doc in enumerate(bad_docs[:10], 1):
        print(f"  {i}. ID={doc['id']}: {doc['url'][:80]}")
    
    if len(bad_docs) > 10:
        print(f"  ... è¿˜æœ‰ {len(bad_docs) - 10} ä¸ª")
    
    print()
    
    # ç¡®è®¤
    response = input(f"â“ ç¡®è®¤è¦åˆ é™¤è¿™ {len(bad_docs)} ä¸ªæ–‡æ¡£å¹¶é‡æ–°çˆ¬å–å—? [y/N] ")
    if response.lower() != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    print()
    
    # 2. æå– URL
    urls = [doc['url'] for doc in bad_docs if doc.get('url')]
    print(f"ğŸ“‹ æå–äº† {len(urls)} ä¸ªæœ‰æ•ˆ URL")
    
    # 3. æ·»åŠ åˆ° Redis é˜Ÿåˆ—å¤´éƒ¨
    add_urls_to_queue_head(urls)
    print()
    
    # 4. åˆ é™¤æ–‡æ¡£
    doc_ids = [doc['id'] for doc in bad_docs]
    await delete_bad_documents(doc_ids)
    print()
    
    print("=" * 70)
    print("âœ… æ¸…ç†å®Œæˆï¼")
    print()
    print("ğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("   1. å¯åŠ¨çˆ¬è™«: cd /home/lancelot/verdant_search/backend/crawler && ./start_crawler.sh")
    print("   2. çˆ¬è™«ä¼šä¼˜å…ˆå¤„ç†è¿™äº› URLï¼ˆå®ƒä»¬åœ¨é˜Ÿåˆ—å¤´éƒ¨ï¼‰")
    print("   3. æµè§ˆå™¨ä¼šæ˜¾ç¤ºï¼ˆheadful æ¨¡å¼ï¼‰ï¼Œå¯ä»¥çœ‹åˆ°çˆ¬å–è¿‡ç¨‹")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(main())
